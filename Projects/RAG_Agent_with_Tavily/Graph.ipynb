{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22a2ce43",
   "metadata": {},
   "source": [
    "## MultiModal RAG Agent with Tavily\n",
    "\n",
    "#### RAG Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "215f751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import uuid\n",
    "from langchain_astradb import AstraDBVectorStore\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated, TypedDict, Literal\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import  AnyMessage\n",
    "import operator\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b469ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Document Loading and Chunking\n",
    "\n",
    "file_path = 'content//attention.pdf'\n",
    "\n",
    "def partition_and_chunk(file_path):\n",
    "    \"\"\"Partitions and chunks a PDF file into manageable pieces. \"\"\"\n",
    "    \n",
    "    chunks = partition_pdf(\n",
    "        filename=file_path,\n",
    "        strategy='hi_res',\n",
    "        infer_table_structure=True,\n",
    "        \n",
    "        extract_image_block_types=['Image'],\n",
    "        extract_image_block_to_payload=True,\n",
    "        \n",
    "        chunking_strategy='by_title',\n",
    "        max_characters = 2000,\n",
    "        combine_text_under_n_chars = 500,\n",
    "        new_after_n_chars = 6000,\n",
    "    )\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "773e8534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    }
   ],
   "source": [
    "chunks = partition_and_chunk(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d08fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract text\n",
    "texts = [chunk for chunk in chunks if 'CompositeElement' in str(type(chunk))]\n",
    "## getting base64 object of image\n",
    "images = [ el.metadata.image_base64 for chunk in chunks for el in chunk.metadata.orig_elements if 'Image' in str(type(el)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2da6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = '''\n",
    "You are an assistant tasked with summarizing tables and text.\n",
    "Give a concise summary of the table or text.\n",
    "\n",
    "Respond only with the summary, no additional comment.\n",
    "Do not start your message by saying \"Here is a summary\" or anything similar.\n",
    "just give the summary as it is.\n",
    "\n",
    "Table or text chunk: {element}\n",
    "'''\n",
    "\n",
    "prompt_image = \"\"\"Describe the image in detail. Be specific about the architecture, graphs, plots such as bar plot\"\"\"\n",
    "image_message = [\n",
    "    (\n",
    "        \"user\",\n",
    "        [\n",
    "         {'type':'text', 'text':prompt_image},\n",
    "         {'type':'image_url','image_url':{'url':'data:image/jpeg;base64,{image}'},}, \n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "llm = ChatOpenAI(model = 'gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fca12c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get summary of text and images\n",
    "def summary_for_vs(chunks, texts = texts, images = images):\n",
    "    \n",
    "    image_prompt_template = ChatPromptTemplate.from_messages(image_message)\n",
    "    image_chain = image_prompt_template | llm | StrOutputParser()\n",
    "    \n",
    "    text_prompy_template = ChatPromptTemplate.from_template(prompt_text)\n",
    "    text_chain = text_prompy_template | llm | StrOutputParser()\n",
    "    \n",
    "    text_summary = text_chain.batch(texts)\n",
    "    image_summary = image_chain.batch(images)\n",
    "    \n",
    "    return text_summary,image_summary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f6e6a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summary, image_summary = summary_for_vs(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ad62141",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = os.getenv('ASTRA_DB_APPLICATION_TOKEN')\n",
    "namespace = os.getenv('ASTRA_DB_KEYSPACE')\n",
    "endpoint = os.getenv('ASTRA_DB_API_ENDPOINT')\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f6192c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The vector store to index the summary chunks\n",
    "vector_store = AstraDBVectorStore(\n",
    "    embedding=embedding,\n",
    "    collection_name=\"RAG_Graph\",\n",
    "    api_endpoint=endpoint,\n",
    "    token=token,\n",
    "    namespace=namespace,\n",
    ")\n",
    "\n",
    "## The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "## The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vector_store,  ## The vector store to index the summary chunks\n",
    "    docstore=store,            ## The storage layer for the parent documents\n",
    "    id_key=id_key,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e825d487",
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding image summries\n",
    "def loading_summaries_to_vector_store(retriever, chunks, chunk_summary):\n",
    "    \"\"\"\n",
    "        Generate ids for each chunk, create langchain document object for each summry chunk.\n",
    "        Indexing the summary in vector store and document in docsotre.\n",
    "    \"\"\"\n",
    "    ## generate unique id for each chunk\n",
    "    doc_ids = [str(uuid.uuid4()) for _ in chunks]\n",
    "    ## Creating Langchain Document objects for each text_summary chunk\n",
    "    summary_texts = [Document(page_content=summary,metadata={id_key:doc_ids[i]}) for i,summary in enumerate(chunk_summary)]\n",
    "    \n",
    "    ## indexing the documents in vector store and document store\n",
    "    retriever.vectorstore.add_documents(summary_texts)\n",
    "    retriever.docstore.mset(list(zip(doc_ids,chunks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5981ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding text summaries to vector store and document store\n",
    "loading_summaries_to_vector_store(retriever,texts,text_summary)\n",
    "## adding image summaries to vector store and document store\n",
    "loading_summaries_to_vector_store(retriever,images,image_summary)\n",
    "## Now the retriever is ready to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95796a9",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79e26618",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Pydantic class\n",
    "\n",
    "class RouterCall_Output(BaseModel):\n",
    "    Topic: Literal['Related','Not Related'] = Field(description=\"Classification of the user query\")\n",
    "    Reasoning: str = Field(description='Reasoning behind topic selection')\n",
    "    \n",
    "parser = PydanticOutputParser(pydantic_object=RouterCall_Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "272e0ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## State class\n",
    "class State(TypedDict):\n",
    "    messages : Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403ed5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisor(state:State):\n",
    "    \n",
    "    question = state['messages']\n",
    "    print(question)\n",
    "    \n",
    "    template = '''\n",
    "    You are an expert in routing. Your task is to classify if the given user query is related to LLM Transformer architecture.\n",
    "    You must classify as either \"Related\" or \"Not Related\".\n",
    "    Provide a brief reasoning for your classification.\n",
    "    \n",
    "    user query : {question}\n",
    "    {format_instructions}\n",
    "    '''\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=['question'],\n",
    "        partial_variables={'format_instructions':parser.get_format_instructions()}\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | parser\n",
    "    response = chain.invoke({'question':question})\n",
    "    print(\"Parsed response:\", response)\n",
    "    \n",
    "    return {'messages':[response.Topic]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "049b1ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Router function\n",
    "def router(state: State):\n",
    "    print('-->Router-->')\n",
    "    \n",
    "    last_message = state['messages'][-1]\n",
    "    print(\"last_message: \",last_message)\n",
    "    \n",
    "    if 'Related' in last_message.lower():\n",
    "        return 'RAG Call'\n",
    "    \n",
    "    return 'LLM Call'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130829ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAG Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f61eecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': 'what are Transformer'}\n",
      "Parsed response: Topic='Related' Reasoning=\"The query 'what are Transformer' is directly asking about the Transformer architecture, which is a foundational concept in LLM (Large Language Model) architectures, particularly in natural language processing.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': ['Related']}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supervisor({'messages':'what are Transformer'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef63339f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
